{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GrqvWOz3cWgC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCxY1-T-cgyE"
   },
   "outputs": [],
   "source": [
    "# No. of classes\n",
    "num_classes = 325\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  #  multi-class classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHpw4OYJcjZq",
    "outputId": "229db16e-a621-4345-a7dc-b9ad2e0d74cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Path: /content/extracted_data/archive/train\n",
      "Test Path: /content/extracted_data/archive/test\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('archive_tamizhi.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/extracted_data')\n",
    "train_path = '/content/extracted_data/archive/train'\n",
    "test_path = '/content/extracted_data/archive/test'\n",
    "\n",
    "\n",
    "print(f\"Train Path: {train_path}\")\n",
    "print(f\"Test Path: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shQd-mPU2hO9"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('Images.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Or9VXv7csCt",
    "outputId": "4c58d5f9-247f-4c79-a8e9-0ddc79c324c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15353 images belonging to 325 classes.\n",
      "Found 3142 images belonging to 325 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 229ms/step - accuracy: 0.0275 - loss: 5.4437 - val_accuracy: 0.4020 - val_loss: 2.3283\n",
      "Epoch 2/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 224ms/step - accuracy: 0.2754 - loss: 2.7920 - val_accuracy: 0.6540 - val_loss: 1.1996\n",
      "Epoch 3/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 223ms/step - accuracy: 0.4631 - loss: 1.7527 - val_accuracy: 0.7896 - val_loss: 0.7817\n",
      "Epoch 4/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 217ms/step - accuracy: 0.5951 - loss: 1.2452 - val_accuracy: 0.8574 - val_loss: 0.6107\n",
      "Epoch 5/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 237ms/step - accuracy: 0.6818 - loss: 0.9527 - val_accuracy: 0.8908 - val_loss: 0.5177\n",
      "Epoch 6/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 220ms/step - accuracy: 0.7341 - loss: 0.7924 - val_accuracy: 0.9074 - val_loss: 0.5464\n",
      "Epoch 7/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 223ms/step - accuracy: 0.7657 - loss: 0.6756 - val_accuracy: 0.9233 - val_loss: 0.4786\n",
      "Epoch 8/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 219ms/step - accuracy: 0.7924 - loss: 0.6103 - val_accuracy: 0.9379 - val_loss: 0.4350\n",
      "Epoch 9/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 230ms/step - accuracy: 0.8256 - loss: 0.5134 - val_accuracy: 0.9411 - val_loss: 0.4586\n",
      "Epoch 10/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 213ms/step - accuracy: 0.8457 - loss: 0.4614 - val_accuracy: 0.9433 - val_loss: 0.4266\n",
      "Epoch 11/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 226ms/step - accuracy: 0.8452 - loss: 0.4352 - val_accuracy: 0.9529 - val_loss: 0.4136\n",
      "Epoch 12/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 224ms/step - accuracy: 0.8579 - loss: 0.4077 - val_accuracy: 0.9488 - val_loss: 0.4384\n",
      "Epoch 13/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 218ms/step - accuracy: 0.8685 - loss: 0.3769 - val_accuracy: 0.9554 - val_loss: 0.4681\n",
      "Epoch 14/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 220ms/step - accuracy: 0.8796 - loss: 0.3439 - val_accuracy: 0.9561 - val_loss: 0.3970\n",
      "Epoch 15/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 222ms/step - accuracy: 0.8885 - loss: 0.3246 - val_accuracy: 0.9539 - val_loss: 0.3773\n",
      "Epoch 16/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 223ms/step - accuracy: 0.8905 - loss: 0.3104 - val_accuracy: 0.9640 - val_loss: 0.4110\n",
      "Epoch 17/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 221ms/step - accuracy: 0.8977 - loss: 0.2916 - val_accuracy: 0.9618 - val_loss: 0.3976\n",
      "Epoch 18/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 227ms/step - accuracy: 0.8983 - loss: 0.2801 - val_accuracy: 0.9650 - val_loss: 0.4025\n",
      "Epoch 19/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 221ms/step - accuracy: 0.9050 - loss: 0.2660 - val_accuracy: 0.9577 - val_loss: 0.4302\n",
      "Epoch 20/20\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 217ms/step - accuracy: 0.9095 - loss: 0.2510 - val_accuracy: 0.9593 - val_loss: 0.4283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fabe6a09fc0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(64, 64),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(64, 64),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "#model training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_generator, epochs=20, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9Z-zaiJc6KP",
    "outputId": "86ee912a-14a6-4166-b4a7-87d11ff05ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9546 - loss: 0.4102\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieUnElj6c8zI",
    "outputId": "0188018d-b6be-4647-be18-dc678e769909"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('tamizhi_ocr_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMbmMy9c6SAP",
    "outputId": "d3acc982-73a4-42e7-e55f-05275a6f0e26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(r\"E:\\Capstone\\Capstone_Tamizhi\\tamizhi_ocr_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WVricbNec_LE"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "id": "GsoWO8VxS4cN",
    "outputId": "abb051da-b6f4-4c4e-e3f6-4c02c7317ba1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     46\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCapstone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCapstone_Tamizhi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mthirukural.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 47\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     48\u001b[0m character_images \u001b[38;5;241m=\u001b[39m segment_characters_without_dots(image)\n\u001b[0;32m     50\u001b[0m plot_characters_dynamically(character_images)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "def segment_characters_without_dots(image):\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    character_images = []\n",
    "\n",
    "    for label in range(1, num_labels):\n",
    "        # bounding box\n",
    "        x, y, w, h, area = stats[label]\n",
    "        if  area < 50 or h < 10 or w < 3   :\n",
    "            continue\n",
    "        char_image = image[y:y+h, x:x+w]\n",
    "        character_images.append((x, char_image))\n",
    "\n",
    "    character_images = sorted(character_images, key=lambda elem: elem[0])\n",
    "    character_images = [img[1] for img in character_images]\n",
    "\n",
    "    return character_images\n",
    "\n",
    "def plot_characters_dynamically(character_images):\n",
    "    num_chars = len(character_images)\n",
    "    if num_chars == 0:\n",
    "        print(\"No characters detected.\")\n",
    "        return\n",
    "\n",
    "    cols = 5\n",
    "    rows = (num_chars + cols - 1) // cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(12, 12), gridspec_kw={'wspace': 0.3, 'hspace': 0.3})\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, char_image in enumerate(character_images):\n",
    "        axs[i].imshow(char_image, cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    for j in range(i + 1, rows * cols):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_path = r'E:\\Capstone\\Capstone_Tamizhi\\Images\\thirukural.png'\n",
    "image = cv2.imread(image_path)\n",
    "character_images = segment_characters_without_dots(image)\n",
    "\n",
    "plot_characters_dynamically(character_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OIDDyWH75WtO"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_character_sequence(image_path, model, class_labels):\n",
    "    image = cv2.imread(image_path)\n",
    "    character_images = segment_characters_without_dots(image)\n",
    "    predicted_sequence = []\n",
    "    for char_img in character_images:\n",
    "        char_img = cv2.resize(char_img, (64, 64))\n",
    "        char_img = char_img.astype('float32') / 255.0\n",
    "        char_img = np.expand_dims(char_img, axis=-1)\n",
    "        char_img = np.expand_dims(char_img, axis=0)\n",
    "        predictions = model.predict(char_img)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        predicted_character = class_labels[predicted_class]\n",
    "        predicted_sequence.append(predicted_character)\n",
    "\n",
    "    sequence = ' '.join(predicted_sequence)\n",
    "    print(f\"Predicted Sequence: {sequence}\")\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fPA8nxJX5b6N"
   },
   "outputs": [],
   "source": [
    "test_folder_path = r'E:\\Capstone\\Capstone_Tamizhi\\archive\\test'\n",
    "class_labels = sorted(os.listdir(test_folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzM5ILOX5e3A",
    "outputId": "f8762aee-5f9c-4b3e-970e-b31f471c7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted Sequence: 93 156 25 17 221 220\n"
     ]
    }
   ],
   "source": [
    "sequence_image_path = r'E:\\Capstone\\Capstone_Tamizhi\\Images\\tholkaappiyam.png'\n",
    "predicted_sequence = predict_character_sequence(sequence_image_path, model, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrOfqPmW5hJV",
    "outputId": "ee85c559-a63b-4233-8971-4a1c7a0ec889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Sequence: 93 156 25 17 221 220\n",
      "Unicode Sequence: தி ர க் கு ற ள்\n"
     ]
    }
   ],
   "source": [
    "file_path = r'E:\\Capstone\\Capstone_Tamizhi\\Unicode_map - unicode_map (2).csv'\n",
    "try:\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path}\")\n",
    "    raise\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The CSV file is empty.\")\n",
    "    raise\n",
    "\n",
    "data.columns = ['Class Name', 'Unicode']\n",
    "data['Unicode'] = data['Unicode'].astype(str).str.strip()\n",
    "\n",
    "unicode_map = {}\n",
    "for _, row in data.iterrows():\n",
    "\n",
    "    class_name = str(row['Class Name'])\n",
    "    #print(class_name)\n",
    "    unicode_value = row['Unicode']\n",
    "    try:\n",
    "        combined_character = \"\".join(\n",
    "            chr(int(code_point[2:], 16)) for code_point in unicode_value.split() if code_point.startswith('U+')\n",
    "        )\n",
    "        unicode_map[class_name] = combined_character\n",
    "    except ValueError:\n",
    "        print(f\"Invalid Unicode format for class {class_name}: '{unicode_value}'\")\n",
    "\n",
    "def get_unicode_sequence(class_names):\n",
    "    class_name_list = class_names.split()\n",
    "    characters = [unicode_map.get(name, \"?\") for name in class_name_list]\n",
    "    return \" \".join(characters)\n",
    "\n",
    "class_name_input = predict_character_sequence(sequence_image_path, model, class_labels)\n",
    "unicode_sequence = get_unicode_sequence(class_name_input)\n",
    "print(f\"Unicode Sequence: {unicode_sequence}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
